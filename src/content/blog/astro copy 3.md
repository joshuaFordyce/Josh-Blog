---
title: 'Comparing the Temporal Robustness of Word Embedding Methods'
description: 'We will focus on comparing frequency-based word embedding methods and easily utilized pre-trained word embedding methods.'
pubDate: 'Jul 02 2022'
heroImage: '../../assets/images/te.jpg'
category: 'Machine Learning'
tags: ['JavaScript', 'css', 'HTML5', 'GitHub']
---

1. Introduction

While Language models have been shown to be effective tools for text classification problems, their performance experiences a drastic drop off in performance when asked to work with temporally distant data. This phenomenon has been especially apparent in sentiment classification tasks Lukes and Sogaard [1]. Text data is highly dynamic and context changes over time. A model that interprets the colloquial terms that depend on context as positives today might assign a false positives to them tomorrow because of the changing context. Therefore, models must be built with this in mind. In this paper we will focus on comparing a frequency-based word embedding method against a pre-trained prediction-based model. Frequency based statistically measures how frequently words appear in the data and then uses these measurements to store semantic information. Prediction based embedding are generated by models that cluster together neighboring words and then predict words from their neighbor clusters. This leads to aligning words with similar contexts together in the embedding space. GloVe (Global Vectors for Word Representation) is based on leveraging matrix factorization on the context matrix by create a matrix of words and their contexts then run least squares regression to compute a low dimensional representation of the embedding plane. In this paper we will explain our approaches to vectorizing textual data and test whether there is a difference in temporal robustness between the two of them. We found that there is a significant gain in robustness when using prediction-based embedding compared to frequency-based word embeddings and further tested the ability of XGBoost to improve upon the temporal robustness that our experimental multinomial Bayes model achieved.

2. Related Work

Recently, Medina-Alias and Simsek [7] showed that there is value in isolating different parts of the model pipeline as the independent variable and experimenting with different amounts of exposure to training data. In another paper, Ninalga [6] focused on pre-fixed data into a pre-trained language model and this partially inspired our hypothesis that narrowing down our research to benchmarking word embeddings can provide significant temporal robustness.
Another team Lukes, Sogaard [1] devised a feature selection approach that induced temporal robustness by computing the lexical polarity for different time periods. Regarding context-aware textual representations, Röttger and Pierrehumbert [2] found that combining temporal information with context aware representations could affect temporal performance.  

3. Task Description

The Clef Longeval Task 2 is a binary sentiment classification challenge. The test data gets more temporally distant. The data comprises of just over 50,000 tweets. Participants were instructed to utilize the Macro-F1 score and the relative performance drop (RPD). The task had three different phases. The development phase allowed participants to develop and pre-evaluate their models. The evaluation phase allowed participants to test their models against the competition test set. The post-evaluation phase was the final phase that allowed the participants to analyze model performance by providing the truth labels of the test sets.  

4. Methodology
4.1 Data Cleaning
We first cleaned our dataset by tokenizing text, removing stop words and finally implementing stemming/lemmatization techniques. We removed URLs, emojis and spelling errors using the NLTK library. We considered removing noise or normalizing the case but considering we want to improve the temporal persistence we tried to minimize the amount of noise removal we did, and we felt that case provided context that may contribute to the temporal performance of the model. We then used the NLTK library and used the tokenize function to help us split our tweets into words. We then went through and removed stop words which are simply words that don’t add much context to the text like conjunctions, prepositions, and pronouns. We did this by filtering the data utilizing the NLTK library stop words list. Finally, we implemented stemming using the Stemmer function from the NLTK library to help us standardize words back to their root forms. 
4.2 Baseline Model
 We developed a baseline classifier comprised of count vectorizer, which vectorized the words, and a multinomial naive bayes model. We performed LaPlace smoothing to regularize our multinomial Naive Bayes. We assigned alpha = 1 to resolve the issue of zero probability in our Naive Bayes model. 
4.3 Experimental Models
We developed our first experimental model by first embedding words using a pre-trained GloVe model and then training an XGBoost model on said embeddings. We tuned the XGBoost using two main methods. The first method we used was tuning gamma which we set to zero. The second method used was finding an optimal learning rate. We tested several learning rates and eventually settled on a learning rate of 0.13. These two methods led to a significant gain in performance. Our second experimental model was developed by embedding words using the same pre-trained GloVe model and then training a Multinomial Naive Bayes model on said embeddings. We choose to perform the same tuning operations on the experimental Multinomial Naive Bayes model as our baseline model so that we could isolate the word embeddings method as our independent variable. 

Table 1
Table title
Model	F1 present	F1 short	F1 long	RPD(Present -Long)
CV + Mn NB	.68	.62	.63	-.07
GloVe + XGB	.73	.73	.72	            -.01
GloVe + Mn NB	.69	.67	.68	-.01

5. Results

We Report the results of the experiments in Table 1. We can see a difference in performance when comparing how well our experimental models are to the baseline. We see that our frequency based embedding count vectorizer performed poorly compared to our pre-trained embedding GloVe. Considering both are context independent it seems like the most significant difference is how the GloVe embeddings are trained. Based on this assumption, we can further hypothesize that pre-trained embeddings can yield significant performance gains.

6. Conclusion

In this paper, we introduce a computationally simplistic framework for mitigating the performance drop-off in sentiment analysis models across temporal distance. Our experiment compared a frequency-based word embedding method with a pre-trained word embedding method. Our experimental models used Glove embeddings to first vectorize our text then train a XGBoost model and a multinomial regression model on these embeddings. Based on the RPD that we recorded, we further our hypothesis that non contextual embeddings can provide immunity from dynamic context changes over time. Further research will be focused on developing a context dependent sentiment analysis classification model and comparing context dependent word embeddings such as ELMo(Embeddings from Language Models) and GPT-2(Generative Pre-Trained Transformer 2) with our current context independent word embeddings. This will allow us to test our hypothesis that adding context to sentiment analysis classifiers can lead to a deterioration in long-term temporal robustness.  



 Acknowledgements

This work was facilitated by the Kaggle Competition Group of Georgia Tech’s Data Science Club

References
